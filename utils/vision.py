# vision.py

import base64
import requests
import io
from PIL import Image
from datetime import datetime
import openai
from config import OPENAI_API_KEY
from utils.db import is_premium, log_image_analysis
from deep_translator import GoogleTranslator
import pytesseract

openai.api_key = OPENAI_API_KEY

# âœ… Ø¶ØºØ· Ø§Ù„ØµÙˆØ±Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…
def compress_image(img_bytes, max_size=(1024, 1024)):
    image = Image.open(io.BytesIO(img_bytes))
    image.thumbnail(max_size)
    output = io.BytesIO()
    image.save(output, format="JPEG", quality=80)
    return output.getvalue()

# âœ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ base64
def encode_image_base64(img_bytes):
    return base64.b64encode(img_bytes).decode('utf-8')

# âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR
def extract_text_ocr(img_bytes):
    try:
        img = Image.open(io.BytesIO(img_bytes))
        return pytesseract.image_to_string(img, lang='eng+ara')
    except:
        return ""

# âœ… ØªØ±Ø¬Ù…Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© â†’ Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù…
def translate_to_arabic_if_needed(text):
    try:
        if any(c.isalpha() for c in text) and not any('\u0600' <= c <= '\u06FF' for c in text):
            return GoogleTranslator(source='auto', target='ar').translate(text)
        return text
    except:
        return text

# âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT-4o Vision
def analyze_image(user_id, image_bytes, file_name="ØµÙˆØ±Ø©"):

    # Ø¶ØºØ· ÙˆØªØ­ÙˆÙŠÙ„ Base64
    compressed = compress_image(image_bytes)
    base64_image = encode_image_base64(compressed)

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    prompt = "Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ£Ø®Ø¨Ø±Ù†ÙŠ Ø¨Ù…Ø§ ØªØ­ØªÙˆÙŠÙ‡ Ø¨Ø§Ø­ØªØ±Ø§Ù. Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠÙ‡Ø§ Ù†ØµØŒ ØµÙÙ‡ Ø£ÙŠØ¶Ù‹Ø§."

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±."},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            temperature=0.5,
            max_tokens=1500
        )

        gpt_response = response.choices[0].message["content"]
        translated = translate_to_arabic_if_needed(gpt_response)

        # OCR
        extracted_text = extract_text_ocr(image_bytes)
        if extracted_text.strip():
            translated_text = translate_to_arabic_if_needed(extracted_text)
            translated += f"\n\nðŸ§¾ Ù†Øµ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±Ø©:\n{translated_text}"

        # Ø³Ø¬Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        log_image_analysis(user_id, file_name, translated)

        return translated

    except Exception as e:
        print("âŒ GPT Vision Error:", e)
        return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©."